{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9431b289",
      "metadata": {},
      "source": [
        "# Device Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "32dcafbc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "92d0f417",
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Running on: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "381f7f4b",
      "metadata": {},
      "source": [
        "# Downloading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3fd6687b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# downloading train_dataset from dataverse.harvard.edu named Chest X-Ray Dataset for Respiratory Disease Classification\n",
        "!wget https://dataverse.harvard.edu/api/access/datafile/5194114"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a0f24144",
      "metadata": {},
      "outputs": [],
      "source": [
        "!mv /content/5194114 /content/5194114.npz #renaming the file to .npz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cabe4204",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "37dfe4b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision import models, transforms\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e00e3afc",
      "metadata": {},
      "outputs": [],
      "source": [
        "CLASS_NAMES  = [\"covid\", \"lung_opacity\", \"normal\", \"viral_pneumonia\", \"tuberculosis\"]\n",
        "NUM_CLASSES  = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8b3d54c",
      "metadata": {},
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b0125798",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ChestXrayNPZDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Keeps .npz file open for the lifetime of the dataset.\n",
        "    Stores direct mmap references to image and label arrays to avoid\n",
        "    repeated key lookup overhead on every __getitem__ call.\n",
        "    \"\"\"\n",
        "    def __init__(self, npz_path, transform=None):\n",
        "        self.transform = transform\n",
        "        self.data      = np.load(npz_path, mmap_mode='r')  # open once, stay open\n",
        "        self.images    = self.data['image']                 # direct array reference, no copy\n",
        "        self.labels    = self.data['image_label']           # direct array reference, no copy\n",
        "        self.length    = self.images.shape[0]               # store length from images directly\n",
        "        print(f\"Dataset ready: {self.length} images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = torch.tensor(np.array(self.images[idx]), dtype=torch.float32)\n",
        "        label = torch.tensor(self.labels[idx].item(), dtype=torch.long)\n",
        "\n",
        "        # Permute (H, W, C) -> (C, H, W)\n",
        "        image = image.permute(2, 0, 1)\n",
        "\n",
        "        # Convert RGB to grayscale (C, H, W) -> (1, H, W)\n",
        "        image = image.mean(dim=0, keepdim=True)\n",
        "\n",
        "        # Normalize to [0, 1]\n",
        "        if image.max() > 1.0:\n",
        "            image = image / 255.0\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def __del__(self):\n",
        "        self.data.close()   # close file handle only when dataset is destroyed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f55ab12c",
      "metadata": {},
      "source": [
        "# Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4b23ee6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.Normalize(mean=[0.485], std=[0.229])\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88629f6a",
      "metadata": {},
      "source": [
        "# Load Data & Split into Train / Val / Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f252a713",
      "metadata": {},
      "outputs": [],
      "source": [
        "NPZ_PATH   = \"/content/5194114.npz\" \n",
        "BATCH_SIZE = 16\n",
        "VAL_SPLIT  = 0.15\n",
        "TEST_SPLIT = 0.10\n",
        "\n",
        "full_dataset = ChestXrayNPZDataset(NPZ_PATH, transform=transform)\n",
        "\n",
        "n       = len(full_dataset)\n",
        "n_val   = int(n * VAL_SPLIT)\n",
        "n_test  = int(n * TEST_SPLIT)\n",
        "n_train = n - n_val - n_test\n",
        "\n",
        "train_ds, val_ds, test_ds = random_split(\n",
        "    full_dataset,\n",
        "    [n_train, n_val, n_test],\n",
        "    generator=torch.Generator().manual_seed(42)   # reproducible split every run\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "print(f\"Split -> Train: {n_train} | Val: {n_val} | Test: {n_test}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f4c0ff9b",
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_image = np.array(full_dataset.images[0], dtype=np.float32)\n",
        "raw_label = int(full_dataset.labels[0].item())\n",
        "\n",
        "raw_image = raw_image / 255.0 if raw_image.max() > 1.0 else raw_image\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(raw_image.mean(axis=-1), cmap='gray')\n",
        "plt.title(f\"Label: {CLASS_NAMES[raw_label]}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79b38090",
      "metadata": {},
      "source": [
        "# Model (ResNet50 adapted for grayscale)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "610797fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ChestXrayResNet(nn.Module):\n",
        "    def __init__(self, num_classes=NUM_CLASSES):\n",
        "        super().__init__()\n",
        "\n",
        "        # Modern API — pretrained=True is deprecated since torchvision 0.13\n",
        "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # Replace first conv: 3 channels -> 1 channel (grayscale)\n",
        "        # CRITICAL FIX: average pretrained RGB weights instead of discarding them\n",
        "        # This preserves the learned edge/texture features from ImageNet\n",
        "        original_conv = self.resnet.conv1\n",
        "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        with torch.no_grad():\n",
        "            self.resnet.conv1.weight = nn.Parameter(\n",
        "                original_conv.weight.mean(dim=1, keepdim=True)\n",
        "            )\n",
        "\n",
        "        # Replace final fully-connected layer for 5-class output\n",
        "        in_features = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(in_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "\n",
        "model = ChestXrayResNet(num_classes=NUM_CLASSES)\n",
        "\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Model ready. Trainable parameters: {trainable:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0e11521",
      "metadata": {},
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "384e231e",
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 50\n",
        "SAVE_PATH  = \"best_model.pth\"\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=NUM_EPOCHS):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
        "\n",
        "    model.to(device)\n",
        "    best_val_loss = float('inf')\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # -------- TRAINING PHASE --------\n",
        "        model.train()\n",
        "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
        "\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)              # forward pass\n",
        "            loss    = criterion(outputs, labels)\n",
        "            loss.backward()                      # backward pass\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss    += loss.item() * images.size(0)\n",
        "            preds          = outputs.argmax(dim=1)\n",
        "            train_correct += (preds == labels).sum().item()\n",
        "            train_total   += labels.size(0)\n",
        "\n",
        "        train_loss /= train_total\n",
        "        train_acc   = train_correct / train_total\n",
        "\n",
        "        # -------- VALIDATION PHASE --------\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "\n",
        "        with torch.no_grad():    # no gradients needed for validation\n",
        "            for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]  \"):\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss    = criterion(outputs, labels)\n",
        "\n",
        "                val_loss    += loss.item() * images.size(0)\n",
        "                preds        = outputs.argmax(dim=1)\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "                val_total   += labels.size(0)\n",
        "\n",
        "        val_loss /= val_total\n",
        "        val_acc   = val_correct / val_total\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1:02d}/{num_epochs} | \"\n",
        "              f\"Train Loss: {train_loss:.4f}  Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f}  Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Save only when validation loss improves (best model, not last epoch)\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), SAVE_PATH)\n",
        "            print(f\"  -> Best model saved (val_loss={val_loss:.4f})\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "history = train_model(model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2627c8d",
      "metadata": {},
      "source": [
        "# Plot Training Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6e2e0b30",
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs_range = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1.plot(epochs_range, history[\"train_loss\"], label=\"Train Loss\")\n",
        "ax1.plot(epochs_range, history[\"val_loss\"],   label=\"Val Loss\")\n",
        "ax1.set_title(\"Loss per Epoch\")\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "ax1.set_ylabel(\"Loss\")\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(epochs_range, history[\"train_acc\"], label=\"Train Acc\")\n",
        "ax2.plot(epochs_range, history[\"val_acc\"],   label=\"Val Acc\")\n",
        "ax2.set_title(\"Accuracy per Epoch\")\n",
        "ax2.set_xlabel(\"Epoch\")\n",
        "ax2.set_ylabel(\"Accuracy\")\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "246ef57e",
      "metadata": {},
      "source": [
        "# Final Evaluation on Held-Out Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "cc915214",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Always load the BEST checkpoint, not the weights from the last epoch\n",
        "model.load_state_dict(torch.load(SAVE_PATH, weights_only=True))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(test_loader, desc=\"Evaluating on test set\"):\n",
        "        outputs = model(images.to(device))\n",
        "        preds   = outputs.argmax(dim=1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "print(\"\\n===== Classification Report =====\")\n",
        "print(classification_report(all_labels, all_preds, target_names=CLASS_NAMES))\n",
        "\n",
        "# Confusion matrix heatmap\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d',\n",
        "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, cmap='Blues')\n",
        "plt.title(\"Confusion Matrix — Test Set\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.xticks(rotation=30, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f0b579b",
      "metadata": {},
      "source": [
        "# Image Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e3382f18",
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_batch(model, dataset, indices, class_names):\n",
        "    \"\"\"\n",
        "    Run inference on multiple images sequentially and display results.\n",
        "    Args:\n",
        "        model:       trained model\n",
        "        dataset:     dataset object to read images from\n",
        "        indices:     list of indices to predict on\n",
        "        class_names: list of class name strings\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(25, 10))  # 2 rows x 5 cols = 10 images\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for plot_idx, data_idx in enumerate(indices):\n",
        "        # Load raw image and true label\n",
        "        raw_image  = np.array(dataset.images[data_idx], dtype=np.float32)\n",
        "        true_label = int(dataset.labels[data_idx].item())\n",
        "\n",
        "        # Preprocess — same pipeline as training\n",
        "        t = torch.from_numpy(raw_image)\n",
        "        t = t.permute(2, 0, 1)           # (H, W, C) -> (C, H, W)\n",
        "        t = t.mean(dim=0, keepdim=True)  # (C, H, W) -> (1, H, W)\n",
        "        if t.max() > 1.0:\n",
        "            t = t / 255.0\n",
        "        t = transform(t).unsqueeze(0).to(device)\n",
        "\n",
        "        # Inference\n",
        "        with torch.no_grad():\n",
        "            probs = torch.softmax(model(t), dim=1)[0]\n",
        "            pred  = probs.argmax().item()\n",
        "\n",
        "        # Plot\n",
        "        correct = pred == true_label\n",
        "        color   = \"green\" if correct else \"red\"\n",
        "        axes[plot_idx].imshow(raw_image.mean(axis=-1), cmap='gray')\n",
        "        axes[plot_idx].set_title(\n",
        "            f\"True:  {class_names[true_label]}\\n\"\n",
        "            f\"Pred:  {class_names[pred]} ({probs[pred]:.1%})\",\n",
        "            color=color, fontsize=10\n",
        "        )\n",
        "        axes[plot_idx].axis('off')\n",
        "\n",
        "        # Print probabilities for each image\n",
        "        print(f\"\\nImage {data_idx}:\")\n",
        "        for i, name in enumerate(class_names):\n",
        "            bar = \"#\" * int(probs[i].item() * 30)\n",
        "            print(f\"  {name:<20}: {probs[i]:.4f}  {bar}\")\n",
        "\n",
        "    plt.suptitle(\"Batch Inference — Green = Correct, Red = Incorrect\", fontsize=13)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "import random\n",
        "predict_batch(model, full_dataset, indices=random.sample(range(len(full_dataset)), 10), class_names=CLASS_NAMES)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
